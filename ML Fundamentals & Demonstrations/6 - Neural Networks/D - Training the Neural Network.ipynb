{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7812dc46-9998-4c19-b072-ef152ebb2611",
   "metadata": {},
   "source": [
    "<h1>Training the Neural Network</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750e45e-1f10-4965-9945-8683f84aff55",
   "metadata": {},
   "source": [
    "<h3>Loss</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6a4c0-2adf-4686-90cf-c3aab1a5e097",
   "metadata": {},
   "source": [
    "<p>In order to train a neural network,</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        We need to define a loss function.<br/>\n",
    "        This is a measure of how far off our neural network is from being perfect.<br/>\n",
    "        When we train the neural network, we are optimizing a loss function.\n",
    "    </li>\n",
    "    <li>\n",
    "        We will use cross entropy as our loss function.<br/>\n",
    "        This is the same as the likelihood we used in logistic regression but is called by a different name in this context.<br/>\n",
    "        We calculate the cross entropy as follows.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "$$\n",
    "cross\\: entropy = \n",
    "\\begin{cases}\n",
    "p & \\text{if y = 1} \\\\\n",
    "1-p & \\text{if y = 0}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<p>Letâ€™s say we have two models to compare on a tiny dataset with 4 datapoints. Here is a table of:</p>\n",
    "<ul>\n",
    "    <li>The true values</li>\n",
    "    <li>The predicted probabilities for model 1</li>\n",
    "    <li>The predicted probabilities for model 2</li>\n",
    "</ul>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Target</th>\n",
    "        <th>Model 1 Prediction</th>\n",
    "        <th>Model 2 Prediction</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>0.6</td>\n",
    "        <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>0.8</td>\n",
    "        <td>0.9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>0.3</td>\n",
    "        <td>0.1</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>0</td>\n",
    "        <td>0.4</td>\n",
    "        <td>0.5</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<p>The cross entropy for model 1 is as follows.</p>\n",
    "$$ 0.5\\:.\\: 0.8\\: .\\: (1 - 0.3)\\: .\\: (1 - 0.4) = 0.2016 $$\n",
    "\n",
    "\n",
    "<p>The cross entropy for model 2 is as follows.</p>\n",
    "$$ 0.5\\:.\\: 0.9\\: .\\: (1 - 0.1)\\: .\\: (1 - 0.5) = 0.2025 $$\n",
    "\n",
    "<strong>\n",
    "    Cross entropy will be higher the better the model is, thus since model 2 has higher cross entropy than model 1, it is the better model.\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b3d75-1cfc-46ce-9ad9-078e7943d673",
   "metadata": {},
   "source": [
    "<h3>Backpropagation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d82095-f0a2-4580-bb7e-6ed7e81b4f64",
   "metadata": {},
   "source": [
    "<p>A neural network has a lot of parameters that we can control.</p>\n",
    "<ul>\n",
    "    <li>There are several coefficients for each node and there can be a lot of nodes!</li>\n",
    "    <li>The process for updating these values to converge on the best possible model is quite complicated.</li>\n",
    "    <li>The neural network works backwards from the output node iteratively updating the coefficients of the nodes.</li>\n",
    "    <li>This process of moving backwards through the neural network is called <strong>backpropagation or backprop</strong>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>The details <strong>backpropagation</strong> behind involves calculating partial derivatives, but the main idea is as follows:</p>\n",
    "<ul>\n",
    "    <li>Before we create a neural network we fix the number of nodes and number of layers.</li>\n",
    "    <li>Then, we initialize all the coefficient values</li>\n",
    "    <li>After that, we iteratively change the values so that at every iteration we see improvement in the loss function.</li>\n",
    "    <li>Eventually we cannot improve the loss function anymore and then we have found our optimal model.</li>\n",
    "</ul>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
