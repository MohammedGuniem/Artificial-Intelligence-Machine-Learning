{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b023429c-2c9e-43eb-bfc0-d85a7f70d806",
   "metadata": {},
   "source": [
    "<h1>Definition of a Random Forest</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcde511-b11f-4647-940b-139e9c057fd2",
   "metadata": {},
   "source": [
    "<h3>Improving on Decision Trees</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a77855-a594-4218-9b32-3372a59e241f",
   "metadata": {},
   "source": [
    "<p>\n",
    "    One of the advantages of decision trees over a model like logistic regression is that they make no assumptions about how the data is structured.<br/>\n",
    "    A decision tree has the potential to get at the essence of the data no matter how it is structured. and whether we can draw a line between the data or not.\n",
    "</p>\n",
    "<p>\n",
    "    However, decision trees have a tendency to overfit, and we can improve them by pruning techniques.\n",
    "</p>\n",
    "<p>\n",
    "    There is also another way of using decision trees to make a better model. which is the <strong>Random Forest</strong> model.<br/>\n",
    "    <strong>Random Forest</strong> is a model built with multiple trees. The goal of random forests is to take the advantages of decision trees while mitigating the variance issues.\n",
    "</p>\n",
    "<p>\n",
    "    <strong>Random Forest</strong> is also known as an ensemble because it uses multiple machine learning models to create a single model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be21aee-2f13-4272-8fcf-94470bd135d1",
   "metadata": {},
   "source": [
    "<h3>Bootstrapping</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb8e37-7b09-438d-aad4-d7ada197b2dc",
   "metadata": {},
   "source": [
    "<p>\n",
    "    A <strong>bootstrapped</strong> sample is a random sample of datapoints where we randomly select with replacement datapoints from our original dataset to create a dataset of the same size.\n",
    "</p>\n",
    "<p>\n",
    "    Keeping in mind that randomly selecting with replacement means that we can choose the same datapoint multiple times, as a result some datapoints from the original dataset will appear multiple times and some will not appear at all.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    For example if we have four datapoints A, B, C, D, the following could be 3 resamples:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>A, A, B, C</li>\n",
    "    <li>B, B, B, D</li>\n",
    "    <li>A, A, C, C</li>\n",
    "</ul>\n",
    "\n",
    "<p>Bootstraping is used to mimic creating multiple samples, as we would rather be able to get more samples of data from the population, but as all we have is our training set, we use that to generate additional datasets.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a8153-7047-48d1-ae58-003f7323d00a",
   "metadata": {},
   "source": [
    "<h3>Bagging Decision Trees</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0eb251-a922-40ea-8069-58d17314665b",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <strong>Bootstrap Aggregation (or Bagging)</strong> is a technique for reducing the variance in an individual model by creating an ensemble from multiple models built on bootstrapped samples.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    To bag decision trees, we do the following:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        we create multiple (say 10) bootstrapped resamples of our training dataset.<br/> Each datapoint count of each resample equals the datapoint count of the original dataset,<br /> but since datapoints are selected randomly from the original dataset, the resample might contain the same datapoint multiple times, and not contain some datapoint at all.\n",
    "    </li>\n",
    "    <li>\n",
    "        We create a decision tree with each of these 10 resamples.\n",
    "    </li>\n",
    "    <li>\n",
    "        To make a prediction, we make a prediction with each of the 10 decision trees and then each decision tree gets a vote.<br/> The prediction with the most votes is the final prediction.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<strong>\n",
    "    When we bootstrap the training set, we're trying to wash out the variance of the decision tree.<br/>The average of several trees that have different training sets will create a model that more accurately gets at the essence of the data.\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b86418-f553-42d4-8aeb-e6c73168ff06",
   "metadata": {},
   "source": [
    "<h3>Decorrelate the Trees</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36721740-e7d2-4af8-a4db-a7d4950bf329",
   "metadata": {},
   "source": [
    "<p>\n",
    "    With bagged decision trees, the trees may still be too similar to have fully created the ideal model.<br/>They are built on different resamples, but they all have access to the same features.<br/>Thus we will add some restrictions to the model when building each decision tree so the trees have more variation. <br/>We call this <strong>decorrelating the trees</strong>.\n",
    "</p>\n",
    "\n",
    "<p>In a decision tree for a random forest:</p>\n",
    "<ul>\n",
    "    <li>At each node, we randomly select a subset of the features to consider.</li>\n",
    "    <li>This will result in us choosing a good, but not the best, feature to split on at each step.</li>\n",
    "    <li>\n",
    "        Itâ€™s important to note that the random selection of features happens at each node.<br/>So maybe at the first node we consider the Sex and Fare features and then at the second node, the Fare and Age features.\n",
    "    </li>\n",
    "    <li>\n",
    "        A standard choice for the number of features to consider at each split is the square root of the number of features.<br/>So if we have 9 features, we will consider 3 of them at each node (randomly chosen).\n",
    "    </li>\n",
    "    <li>If we bag these decision trees, we get a random forest.</li>\n",
    "</ul>\n",
    "\n",
    "<strong>\n",
    "    Each decision tree within a random forest is probably worse than a standard decision tree. But when we average them we get a very strong model!\n",
    "</strong>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
