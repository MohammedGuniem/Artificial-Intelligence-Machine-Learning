{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bebf91a-da5d-4d05-ab4c-a22e8e6ec7e2",
   "metadata": {},
   "source": [
    "<h1>Evaluation Metrics</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ae5ae-573e-4513-b719-8ecc0caac276",
   "metadata": {},
   "source": [
    "<h3>Accuracy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf652d80-bbeb-4ffd-bf49-c8dadc99b8b4",
   "metadata": {},
   "source": [
    "<p>Accuracy is the percent of predictions that are correct.</p>\n",
    "<p>For example, if you have 100 datapoints and predict 70 of them correctly and 30 incorrectly, the accuracy is 70%.</p>\n",
    "\n",
    "<p>Accuracy is a very straightforward and easy to understand metric, however it’s not always the best one.</p>\n",
    "\n",
    "<p><strong>For example,</strong> let’s say I have a model to predict whether a credit card charge is fraudulent. Of 10000 credit card chards, we have 9900 legitimate charges and 100 fraudulent charges. I could build a model that just predicts that every single charge is legitimate and it would get 9900/10000 (99%) of the predictions correct!</p>\n",
    "\n",
    "<p><strong>Another example,</strong> let’s say you have a model to predict spam email. Your training set has 1000 emails, 950 are legitimate emails and 50 are spam emails. If you build a model that just predicts every email is legitimate and then you get 950/1000 (95%) accuracy score</p>\n",
    "\n",
    "<strong>This means accuracy is a good measure if our classes are evenly split, but is very misleading if we have imbalanced classes.</strong>\n",
    "\n",
    "<p>Therefore always use caution with accuracy. You need to know the distribution of the classes to know how to interpret the value</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b681693-a431-4171-b88c-523a51342ccb",
   "metadata": {},
   "source": [
    "<h3>Confusion Matrix</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a2378-39e6-43eb-b085-2b8283784e35",
   "metadata": {},
   "source": [
    "<p>As we noticed in the previous part, we care not only about how many datapoints we predict the correct class for, we care about how many of the positive datapoints we predict correctly for as well as how many of the negative datapoints we predict correctly.</p>\n",
    "\n",
    "<p>We can see all the important values in what is called the Confusion Matrix (also called the Error Matrix or the Table of Confusion).</p>\n",
    "\n",
    "<p>The Confusion Matrix is a table showing four values:</p>\n",
    "<ul>\n",
    "    <li><strong>True Positive (TP):</strong> Datapoints we predicted positive that are actually positive.</li>\n",
    "    <li><strong>False Negative (FN):</strong> Datapoints we predicted positive that are actually negative.</li>\n",
    "    <li><strong>False Positive (FP)</strong> Datapoints we predicted negative that are actually positive.</li>\n",
    "    <li><strong>True Negative (TN)</strong> Datapoints we predicted negative that are actually negative.</li>\n",
    "</ul>\n",
    "\n",
    "<p>The terms can be a little hard to keep track of. The way to remember is that the second word is what our prediction is (positive or negative) and the first word is whether that prediction was correct (true or false).</p>\n",
    "\n",
    "<p>In our Titanic dataset, we have 887 passengers, 342 survived (positive) and 545 didn’t survive (negative). The model we built in the previous module has the following confusion matrix.</p>\n",
    "\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "      <th></th>\n",
    "      <th>Actual Positive</th>\n",
    "      <th>Actual Negative</th>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: white;\">\n",
    "      <th>Predicted Positive</th>\n",
    "      <td style=\"background-color: lightblue;\">233</td>\n",
    "      <td>65</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <th>Predicted Negative</th>\n",
    "      <td>109</td>\n",
    "      <td style=\"background-color: lightblue;\">480</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<p>The blue shaded squares are the counts of the predictions that we got correct. So of the 342 passengers that survived, we predicted 233 or them correctly (and 109 of them incorrectly). Of the 545 passengers that didn’t survive, we predicted 480 correctly (and 65 incorrectly).</p>\n",
    "\n",
    "<p>We can use the confusion matrix to compute the accuracy. As a reminder, the accuracy is the number of datapoints predicted correctly divided by the total number of datapoints</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e7851a-fc29-46b1-b28c-cea21609ecac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8038331454340474\n"
     ]
    }
   ],
   "source": [
    "print((233+480)/(233+65+109+480))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d148d2-10e1-4ac4-8a24-92c8818e3af9",
   "metadata": {},
   "source": [
    "<strong>The confusion matrix fully describes how a model performs on a dataset, though is difficult to use to compare models.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f954a-6446-4d5d-b979-2c1d62da0ffd",
   "metadata": {},
   "source": [
    "<h3>True Positives, True Negatives, False Positives, False Negatives</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ce342-7173-43ff-a7aa-32bac728b793",
   "metadata": {},
   "source": [
    "<p>The confusion matrix is described as follows:</p>\n",
    "\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "      <th></th>\n",
    "      <th>Actual Positive</th>\n",
    "      <th>Actual Negative</th>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: white;\">\n",
    "      <th>Predicted Positive</th>\n",
    "      <td style=\"background-color: lightblue;\">TP</td>\n",
    "      <td>FP</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <th>Predicted Negative</th>\n",
    "      <td>FN</td>\n",
    "      <td style=\"background-color: lightblue;\">TN</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<p>The four values of the confusion matrix (TP, TN, FP, FN) are used to compute several different metrics that we’ll use later on.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
