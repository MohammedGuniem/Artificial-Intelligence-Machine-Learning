{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a50bed-f39d-4c8e-bb8a-da866a82e948",
   "metadata": {},
   "source": [
    "<h1>Training and Testing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d2fd0-e764-4d36-b54e-69bc5052c6a9",
   "metadata": {},
   "source": [
    "<h3>Overfitting</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f333d5-3f78-4adb-919c-7349d478574a",
   "metadata": {},
   "source": [
    "<p>Building a model with all of our data and then looking on how well it performs on the same data artificially inflate our numbers since our model, in effect, got to see the answers to the quiz before we gave it the quiz, which can lead to what we call overfitting</p>\n",
    "\n",
    "<strong>Overfitting is when we perform well on the data the model has already seen, but we don’t perform well on new data.</strong>\n",
    "\n",
    "<p>We can visualize overfitting by the line drawn in the plot image below, where the line is too closely trying to get every single datapoint on the correct side of the line but it is missing the essence of the data.</p>\n",
    "\n",
    "![Overfitting Example Image](./images/overfitting.jpg)\n",
    "\n",
    "<p>In the graph you can see that we’ve done a pretty good job of getting the yellow dots on the top and the purple dots on the bottom, but it isn’t capturing what’s going on.</p>\n",
    "\n",
    "<p>A single outlier point could really throw off the location of the line. While the model would get a great score on the data it’s already seen, it’s unlikely to perform well on new data.</p>\n",
    "\n",
    "<strong>It is worth it to note that the more features we have in our dataset, the more prone we’ll be to overfitting.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf0708-3723-4172-8d74-81b13d700fd3",
   "metadata": {},
   "source": [
    "<h3>Training Set and Test Set</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76264266-3bd5-484b-8b3f-c7f1c29826db",
   "metadata": {},
   "source": [
    "<p>In action, our model will be making predictions on data we don’t know the answer to, so we’d like to evaluate how well our model does on new data, not just the data it’s already seen.</p>\n",
    "\n",
    "<p>To simulate making predictions on new unseen data, we can break our dataset into 2 sets:</p>\n",
    "<ul>\n",
    "    <li>Training Set: Which is used for building the models.</li>\n",
    "    <li>Test Set: Which is used for evaluating the models.</li>\n",
    "</ul>\n",
    "\n",
    "<p>We split our data before building the model, thus the model has no knowledge of the test set and we’ll be giving it a fair assessment</p>\n",
    "\n",
    "<p>An example is if our dataset has 200 datapoints in it, breaking it into a training set and test set might look as follows.</p>\n",
    "\n",
    "![Training and Testing Dataset Split Example Image](./images/training_and_testing_sets.jpg)\n",
    "\n",
    "<strong>A standard breakdown is to put 70-80% of our data in the training set and 20-30% in the test set. Using less data in the training set means that our model won’t have as much data to learn from, so we want to give it as much as possible while still leaving enough for evaluation.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcecc1f8-bcb7-473f-8ab5-941fcf3b25bd",
   "metadata": {},
   "source": [
    "<h3>Training and Testing in Sklearn</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab5b5d-4b52-419b-9812-adb1fe8074cb",
   "metadata": {},
   "source": [
    "<p>Scikit-learn has a function built in for splitting the data into a training set and a test set. This function is called <strong>train_test_split</strong></p>\n",
    "<p>This function will randomly put each datapoint in either the training set or the test set. By default the training set is 75% of the data and the test set is the remaining 25% of the data.</p>\n",
    "\n",
    "<p>The code below demonstrate how to split our data into a training and a test dataset using scikit-learn</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed186ac0-cdc4-476f-8e3e-2f4e823599bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of whole dataset: (887, 6) (887,)\n",
      "Shape of training set: (665, 6) (665,)\n",
      "Shape of test set: (222, 6) (222,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(\"Shape of whole dataset:\", X.shape, y.shape)\n",
    "print(\"Shape of training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Shape of test set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43867ea-544d-4135-9ab0-6a0e0af2af36",
   "metadata": {},
   "source": [
    "<p>We can see the following:</p>\n",
    "<ul>\n",
    "    <li>Of the 887 datapoints in our dataset, 665 of them are in our training set and 222 are in the test set.</li>\n",
    "    <li>Every datapoint from our dataset is used exactly once, either in the training set or the test set.</li>\n",
    "    <li>We have 6 features in our dataset, so we still have 6 features in both our training set and test set.</li>\n",
    "</ul>\n",
    "\n",
    "<p>We can change the size of our training set by using the <strong>train_size</strong> parameter. E.g. train_test_split(X, y, <strong>train_size=0.6</strong>) would put <strong>60% of the data</strong> in the training set and 40% in the test set.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa5b85-77d1-46ed-bb76-18654e8de39c",
   "metadata": {},
   "source": [
    "<h3>Building a Scikit-learn Model Using a Training Set</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff6bc1-93ad-4b59-91a4-0e470b66b36e",
   "metadata": {},
   "source": [
    "<p>Now we can build the model using the training set, and then evaluate it by calculating the evaluation metrics on the test set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8bcc27c-e935-42a5-a1c1-8ebb24d7c69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7882882882882883\n",
      "precision: 0.7228915662650602\n",
      "recall: 0.7142857142857143\n",
      "f1 score: 0.718562874251497\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# building the model\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluating the model\n",
    "# print(\"accuracy:\", model.score(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74567a7d-5e8f-4b07-95cf-7be58f65a6c6",
   "metadata": {},
   "source": [
    "<strong>Note that our accuracy, precision, recall and F1 score values are actually very similar to the values when we used the entire dataset. This is a sign that our model is not overfit!</strong>\n",
    "\n",
    "<p>Also note if you run the code, you’ll notice that you get different scores each time. This is because the train test split is done randomly, and depending which points land in the training set and test, the scores will be different. Although you can also use the <strong>random_state</strong> parameter and give it a arbitrary number to get the same random split each time you run the code above, or you can use <strong>cross validation</strong> for more accurate means of measuring these scores.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ad5cf-e607-44fa-8d05-374f20eb8a26",
   "metadata": {},
   "source": [
    "<h3>Using a Random State</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ed871-6a1d-4dcf-8859-d81970ccfcb9",
   "metadata": {},
   "source": [
    "<p>Each time we run the previous code, we will get different results. This is a result of randomness, and we need it to be random for it to be effective, but this can sometimes make it difficult to test the code.</p>\n",
    "\n",
    "<p>To get the same split every time, we can use the <strong>random_state</strong> attribute. We choose an arbitrary number to give it, and then every time we run the code, we will get the same split as demonstrated below.</p>\n",
    "\n",
    "<p>The random state is also called a seed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7e6e2b-32a8-43ff-b25f-f550331c5363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: [[3 True 28.0 0 0 7.7958]\n",
      " [3 False 22.0 0 0 10.5167]\n",
      " [1 True 24.0 0 0 79.2]\n",
      " ...\n",
      " [3 False 26.0 1 0 15.5]\n",
      " [1 True 42.0 1 0 52.0]\n",
      " [3 False 0.75 2 1 19.2583]]\n",
      "X_test: [[3 False 39.0 1 5 31.275]\n",
      " [2 True 34.0 0 0 13.0]\n",
      " [1 True 37.0 0 1 29.7]\n",
      " ...\n",
      " [1 True 28.0 0 0 35.5]\n",
      " [1 False 54.0 1 0 78.2667]\n",
      " [2 True 21.0 1 0 11.5]]\n",
      "accuracy: 0.7657657657657657\n",
      "precision: 0.7619047619047619\n",
      "recall: 0.6666666666666666\n",
      "f1 score: 0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)\n",
    "print('X_train:', X_train)\n",
    "print('X_test:', X_test)\n",
    "\n",
    "# evaluating the model\n",
    "# print(\"accuracy:\", model.score(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
