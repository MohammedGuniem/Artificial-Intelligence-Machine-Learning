{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5928597f-d17b-417a-b03d-8aa5a4f6be2c",
   "metadata": {},
   "source": [
    "<h1>k-fold Cross Validation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb14de-0185-4773-9b4f-dd743e76f008",
   "metadata": {},
   "source": [
    "<h3>Concerns with Training and Test Set</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f9f52-659d-405e-aaf7-98f3e638e89c",
   "metadata": {},
   "source": [
    "<p>We are doing evaluation because we want to get an accurate measure of how well the model performs. If our dataset is small, our test set is going to be small. Thus it might not be a good random assortment of datapoints and by random chance end up with easy or difficult datapoints in our evaluation set.</p>\n",
    "\n",
    "<p>\n",
    "    If code below splits the data randomly between 75% for building a model, then evaluate it using the rest 25% of the original datset mulitple time in a loop.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08389020-acf0-436a-9aa5-cdc92c1337d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Run nr.: 0\n",
      " accuracy: 0.81532\n",
      "precision: 0.75000\n",
      "   recall: 0.68000\n",
      " f1 score: 0.71329 \n",
      "\n",
      "*Run nr.: 1\n",
      " accuracy: 0.79279\n",
      "precision: 0.85526\n",
      "   recall: 0.65000\n",
      " f1 score: 0.73864 \n",
      "\n",
      "*Run nr.: 2\n",
      " accuracy: 0.81081\n",
      "precision: 0.72840\n",
      "   recall: 0.74684\n",
      " f1 score: 0.73750 \n",
      "\n",
      "*Run nr.: 3\n",
      " accuracy: 0.81532\n",
      "precision: 0.80723\n",
      "   recall: 0.72826\n",
      " f1 score: 0.76571 \n",
      "\n",
      "*Run nr.: 4\n",
      " accuracy: 0.79730\n",
      "precision: 0.75342\n",
      "   recall: 0.67073\n",
      " f1 score: 0.70968 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "for i in range(0, 5):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    # building the model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluating the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"*Run nr.: {i}\")\n",
    "    print(\" accuracy: {0:.5f}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"precision: {0:.5f}\".format(precision_score(y_test, y_pred)))\n",
    "    print(\"   recall: {0:.5f}\".format(recall_score(y_test, y_pred)))\n",
    "    print(\" f1 score: {0:.5f}\".format(f1_score(y_test, y_pred)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506107ca-ef31-406b-9ad5-6b558d2f7de1",
   "metadata": {},
   "source": [
    "<p>\n",
    "    You can see that each time we run it, we get different values for the metrics.\n",
    "</p>\n",
    "<ul>\n",
    "    <li>The accuracy ranges from 0.79 to 0.82</li>\n",
    "    <li>The precision ranges from 0.72 to 0.86</li>\n",
    "    <li>The recall ranges from 0.65 to 0.74</li>\n",
    "    <li>The f1 score ranges from 0.71 to 0.77</li>\n",
    "</ul>\n",
    "<p>\n",
    "    These are wide ranges that just depend on how lucky or unlucky we were in which datapoints ended up in the test set.\n",
    "</p>\n",
    "<p>\n",
    "    Since our goal is to get the best possible measure of our metrics (accuracy, precision, recall and F1 score), we can do a little better than just a single training and test set. So instead of doing a single train/test split, we’ll split our data into a training set and test set multiple times.\n",
    "</p>\n",
    "\n",
    "<strong>Conslusion! Splitting the dataset into a single training set and test set for evaluation purposes might yield an inaccurate measure of the evaluation metrics when the dataset is small.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb7901-2454-45ac-9b86-05f48d744557",
   "metadata": {},
   "source": [
    "<h3>Multiple Training and Test Sets</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e5532-e319-49d4-84c1-273b59ce0436",
   "metadata": {},
   "source": [
    "<p>\n",
    "    We want to get a measure of how well our model does in general, not just a measure of how well it does on one specific test set. We approach this by doing the following:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>Let’s assume we have 200 datapoints in our dataset</li>\n",
    "    <li>We break our dataset into 5 chunks</li>\n",
    "    <li>Each of these 5 chunks will serve as a test set. When Chunk 1 is the test set, we use the remaining 4 chunks as the training set.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Thus we have 5 training and test sets as follows.</p>\n",
    "\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Split nr.</th>\n",
    "        <th></th>\n",
    "        <th></th>\n",
    "        <th></th>\n",
    "        <th></th>\n",
    "        <th></th>\n",
    "        <th>Accuracy</th>\n",
    "    </tr>\n",
    "  <tr>\n",
    "      <td style=\"background-color: white;\">1</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: yellow;\">Test</td>\n",
    "      <td style=\"background-color: white;\">0.83</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td style=\"background-color: white;\">2</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: yellow;\">Test</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: white;\">0.79</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td style=\"background-color: white;\">3</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: yellow;\">Test</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: white;\">0.78</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td style=\"background-color: white;\">4</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: yellow;\">Test</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: white;\">0.80</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td style=\"background-color: white;\">5</td>\n",
    "      <td style=\"background-color: yellow;\">Test</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: lightblue;\">Train</td>\n",
    "      <td style=\"background-color: white;\">0.75</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<strong>In each of the 5 splits we have a test set of 20% (40 datapoints) and a training set of 80% (160 datapoints). And every datapoint is in exactly 1 test set.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11571e8d-b17d-4694-ba01-805625c01b86",
   "metadata": {},
   "source": [
    "<h3>Building and Evaluating with Multiple Training and Test Sets</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e3fbe-38f3-4590-91ac-eb7777d66e8c",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Now, for each training set, we build a model and evaluate it using the associated test set. Thus we build 5 models and calculate 5 scores.</li>\n",
    "    <li>Then we report the accuracy as the mean of the 5 values:</li>\n",
    "</ul>\n",
    "$$Accuracy = \\frac{0.83+0.79+0.78+0.80+0.75}{5} = 0.79$$\n",
    "\n",
    "<p>If we had just done a single training and test set and had randomly gotten the first one, we would have reported an accuracy of 0.83.</p> <p>If we had randomly gotten the last one, we would have reported an accuracy of 0.75.</p><p>Averaging all these possible values helps eliminate the impact of which test set a datapoint lands in.</p>\n",
    "\n",
    "<p>This process for creating multiple training and test sets is called <strong>k-fold cross validation</strong>. The k is the number of chunks we split our dataset into. The standard number is 5, as we did in our example above.</p>\n",
    "\n",
    "<p>Our goal in cross validation is to get accurate measures for our metrics (accuracy, precision, recall). We are building extra models in order to feel confident in the numbers we calculate and report.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127351c8-d738-4ede-998e-b318ac6d0412",
   "metadata": {},
   "source": [
    "<h3>Final Model Choice in k-fold Cross Validation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae863f5-5428-4620-bc73-a54ee0f83a00",
   "metadata": {},
   "source": [
    "<p>\n",
    "    These 5 models were built just for evaluation purposes, so that we can report the metric values. We don’t actually need these models and want to build the best possible model.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    The best possible model is going to be a model that uses all of the data. So we keep track of our calculated values for our evaluation metrics and then build a model using all of the data.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    This may seem incredibly wasteful, but computers have a lot of computation power, so it’s worth using a little extra to make sure we’re reporting the right values for our evaluation metrics. We’ll be using these values to make decisions, so calculating them correctly is very important.\n",
    "</p>\n",
    "\n",
    "<strong>\n",
    "    Computation power for building a model can be a concern when the dataset is large. In these cases, we just do a train test split.\n",
    "</strong>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
