{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb6eb2d-3f6e-4e07-9ba7-23c5eb3f17c7",
   "metadata": {},
   "source": [
    "<h1>Foundations for the ROC Curve</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5408f70-c8b1-466a-bf5b-2131f27785ec",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression Threshold</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dde76a-f2e0-46c3-855d-dedce824cde7",
   "metadata": {},
   "source": [
    "<p>A logistic regression model offers an easy way of shifting between emphasizing precision and emphasizing recall. This is because the Logistic Regression model doesn’t just return a prediction, but it returns a probability value between 0 and 1.</p>\n",
    "<p>The default threshold is set to 0.5. However, we could choose any threshold between 0 and 1.</p>\n",
    "<p>We have the following adjustement alternatives and their impact:</p>\n",
    "<ul>\n",
    "    <li>Make the threshold higher -> Fewer Positive Predictions -> Positive Predictions More Likely to Be Correct -> Higher Precision and Lower Recall</li>\n",
    "    <li>Make the threshold lower -> More Positive Predictions -> Positive Predictions Less Likely to Be Correct -> Lower Precision and Higher Recall</li>\n",
    "</ul>\n",
    "\n",
    "<p>Each choice of a threshold is a different model. <strong>An ROC (Receiver operating characteristic) Curve</strong> is a graph showing all of the possible models and their performance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f46b4-45e3-475d-a848-93496042ce60",
   "metadata": {},
   "source": [
    "<h3>Sensitivity & Specificity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae38bbe-a754-4a78-8633-0eb95e0892c0",
   "metadata": {},
   "source": [
    "<p>An ROC Curve is a graph of the sensitivity vs. the specificity. These values demonstrate the same trade-off that precision and recall demonstrate.</p>\n",
    "\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "      <th></th>\n",
    "      <th>Actual Positive</th>\n",
    "      <th>Actual Negative</th>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: white;\">\n",
    "      <th>Predicted Positive</th>\n",
    "      <td style=\"background-color: lightblue;\">TP</td>\n",
    "      <td>FP</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <th>Predicted Negative</th>\n",
    "      <td>FN</td>\n",
    "      <td style=\"background-color: lightblue;\">TN</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<p>An example is given in the confusion matrix below</p>\n",
    "\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "      <th></th>\n",
    "      <th>Actual Positive</th>\n",
    "      <th>Actual Negative</th>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: white;\">\n",
    "      <th>Predicted Positive</th>\n",
    "      <td style=\"background-color: lightblue;\">61</td>\n",
    "      <td>21</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <th>Predicted Negative</th>\n",
    "      <td>35</td>\n",
    "      <td style=\"background-color: lightblue;\">105</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<p>The sensitivity is another term for the recall, which is the true positive rate.</p>\n",
    "\n",
    "$$ Sensitivity = Recall = \\frac{\\#\\: positives\\: predicted\\: correctly}{\\#\\: positive\\: cases} = \\frac{TP}{TP+FN} = \\frac{61}{61+35} = \\frac{61}{96} = 0,64$$\n",
    "\n",
    "<p>The specificity is the true negative rate. It’s calculated as follows.</p>\n",
    "\n",
    "$$ Specificity = \\frac{\\#\\: negatives\\: predicted\\: correctly}{\\#\\: negatives\\: cases} = \\frac{TN}{TN+FP} = \\frac{105}{105+21} = \\frac{105}{126} = 0,83$$\n",
    "\n",
    "<strong>The goal is to maximize these two values, though generally making one larger makes the other lower. It will depend on the situation whether we put more emphasis on sensitivity or specificity.</strong>\n",
    "\n",
    "<p>The standard is to build a sensitivity vs. specificity curve, although it is also possible to build a precision-recall curve, but this isn’t commonly done.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33625a-3155-4832-ac23-98d44f810d8d",
   "metadata": {},
   "source": [
    "<h3>Sensitivity & Specificity in Scikit-learn</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c390d784-1370-4e16-b6af-d6d1fd3560eb",
   "metadata": {},
   "source": [
    "<p>Scikit-learn has not defined functions for sensitivity and specificity, but we can do it ourselves.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f475a7f8-83c3-4b3d-bab1-da99a135cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c192fa-76a6-47f0-b3f4-eecf888d08b3",
   "metadata": {},
   "source": [
    "<strong>Sensitivity is the same as recall</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e8bf91-8bff-4e46-be75-ac15b44f5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_score = recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6879aa5-3245-48cb-a31c-c79f7a4459da",
   "metadata": {},
   "source": [
    "<p>Now, to define specificity, if we realize that it is also the recall of the negative class, we can get the value from the sklearn function precision_recall_fscore_support.</p>\n",
    "\n",
    "<p>The second array is the recall, so we can ignore the other three arrays. </p>\n",
    "\n",
    "<p>There are two values:</p>\n",
    "<ul>\n",
    "    <li>The first is the recall of the negative class which is the specificity.</li>\n",
    "    <li>The second is the recall of the positive class which is the standard recall or sensitivity value.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0978fa75-fb1e-454e-8614-e23f7b115139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity_score(y_true, y_pred):\n",
    "    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "    return r[0] # specificity=r[0] and sensitivity=recall=r[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eeba45-c9f3-40d2-b544-3a5c20f5ee87",
   "metadata": {},
   "source": [
    "<p>Now lets use our defined functions sensitivity_score and specificity_score on a model to view the results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af7a4c5-a8d1-4ec7-a830-2885d36b4b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity: 0.6829268292682927\n",
      "specificity: 0.9214285714285714\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"sensitivity:\", sensitivity_score(y_test, y_pred))\n",
    "print(\"specificity:\", specificity_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa0873-2a2d-4491-a933-6ccbd201da74",
   "metadata": {},
   "source": [
    "<strong>Conclusion! Sensitivity is the same as the recall (or recall of the positive class) and specificity is the recall of the negative class.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745bf0c-678c-41e5-b6a0-f1c20f2c6587",
   "metadata": {},
   "source": [
    "<h3>Adjusting the Logistic Regression Threshold in Sklearn</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e9efe-bb19-4bd5-8503-c4cc4a9bd5ac",
   "metadata": {},
   "source": [
    "<p>The predict method of the logistic regression model gets a probability value behind the scene that is between 0 and 1, and if we want to choose a different threshold besides 0.5, we’ll want those probability values. To get these values, we can use the <strong>predict_proba</strong> function.</p>\n",
    "\n",
    "<p>The result is a numpy array with 2 values for each datapoint (e.g. [0.78, 0.22]). You’ll notice that the two values sum to 1.</p>\n",
    "<ul>\n",
    "    <li>The first value is the probability that the datapoint is in the 0 class (didn’t survive)</li>\n",
    "    <li>The second is the probability that the datapoint is in the 1 class (survived)</li>\n",
    "</ul>\n",
    "<p>We only need the second column of this result, which we can pull with the following numpy syntax.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152b2546-d991-41cf-a387-676a41dac68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False False False  True  True False False False\n",
      "  True False False False  True False  True False False False False False\n",
      " False  True False False False False False  True False False False False\n",
      "  True False False False  True False False False False False False False\n",
      " False False False False False False False False  True  True False False\n",
      " False  True False False False False False False False False False False\n",
      "  True False False  True  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False  True False False False  True  True False False\n",
      " False False False  True False False False False False  True False False\n",
      " False  True False False False False False False False False False False\n",
      " False  True False False False False False False False False False  True\n",
      "  True False False  True False False False False False False False False\n",
      " False False False False  True False  True False False False  True False\n",
      " False False  True  True False False False  True False  True False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False  True  True False  True\n",
      " False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_proba(X_test)[:, 1] > 0.75\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317915ca-d9bd-4dde-960d-52010908096d",
   "metadata": {},
   "source": [
    "<strong>A threshold of 0.75 means we need to be more confident in order to make a positive prediction. This results in fewer positive predictions and more negative predictions.</strong>\n",
    "\n",
    "<p>Now we can use any scikit-learn metrics from before using y_test as our true values and y_pred as our predicted values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a2751e0-6bde-4d53-9960-607e69405744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9230769230769231\n",
      "recall: 0.43902439024390244\n",
      "sensitivity: 0.43902439024390244\n",
      "specificity: 0.9785714285714285\n"
     ]
    }
   ],
   "source": [
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n",
    "print(\"sensitivity:\", sensitivity_score(y_test, y_pred))\n",
    "print(\"specificity:\", specificity_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e142b0-7bd0-455e-a3e4-19500c46503c",
   "metadata": {},
   "source": [
    "<strong>Note! that when we increased the threshold value, we got a lower value of sensitivity/recall than in the original model where the default threshold was set to 0.5</strong>\n",
    "\n",
    "<p>Setting the threshold to 0.5 we would get the original Logistic Regression model. Any other threshold value yields an alternative model.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
