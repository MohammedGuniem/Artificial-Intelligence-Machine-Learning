{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec707346-436a-4fda-a671-cf2390ae9c98",
   "metadata": {},
   "source": [
    "<h1>Feature Importances</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489de41b-9cea-4b05-9a5b-b5aca709066c",
   "metadata": {},
   "source": [
    "<p>\n",
    "    We often wants to know if every feature contribute equally to building a model, and if not, which subset of features should we use?<br/>\n",
    "    <strong>Which is what we call feature selection.</strong>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    Mean decrease impurity.<br/>\n",
    "    Recall that a random forest consists of many decision trees, and that for each tree, the node is chosen to split the dataset based on maximum decrease in impurity, typically either Gini impurity or entropy in classification.<br/>\n",
    "    Thus for a tree, it can be computed how much impurity each feature decreases in a tree.<br/>\n",
    "    And then for a forest, the impurity decrease from each feature can be averaged.<br/>\n",
    "    Consider this measure a metric of importance of each feature, we then can rank and select the features according to feature importance.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    Scikit-learn provides a <strong>feature_importances_</strong> variable with the model, which shows the relative importance of each feature. The scores are scaled down so that the sum of all scores is 1.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a93123a-5465-4398-8e97-6fd401e433dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst radius           0.162142\n",
      "worst area             0.136277\n",
      "mean concave points    0.132861\n",
      "mean radius            0.089364\n",
      "mean area              0.087997\n",
      "worst perimeter        0.068513\n",
      "mean concavity         0.065367\n",
      "worst concavity        0.060150\n",
      "radius error           0.030149\n",
      "area error             0.023505\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer_data = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])\n",
    "df['target'] = cancer_data['target']\n",
    "\n",
    "X = df[cancer_data.feature_names].values\n",
    "y = df['target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, random_state=111)\n",
    "rf = RandomForestClassifier(n_estimators=15, random_state=111)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "ft_imp = pd.Series(rf.feature_importances_, index=cancer_data.feature_names).sort_values(ascending=False)\n",
    "print(ft_imp.head(10))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d264f-76cb-4496-becf-ac9678763d41",
   "metadata": {},
   "source": [
    "<p>\n",
    "    From the output, we can see that among all features, <strong>worst radius</strong> is most important (0.31), followed by <strong>worst area</strong> and <strong>worst concave points</strong>.\n",
    "</p>\n",
    "\n",
    "<strong>Note! In regression, we calculate the feature importance using variance instead.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bce1f9-50dc-4a12-abfa-8115f05ae71c",
   "metadata": {},
   "source": [
    "<h3>New Model on Selected Features</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd27182-ddbf-4a0e-8c36-2e0fa7877e5b",
   "metadata": {},
   "source": [
    "<p>Why should we perform feature selection?</p>\n",
    "<ul>\n",
    "    <li>it enables us to train a model faster</li>\n",
    "    <li>it reduces the complexity of a model thus makes it easier to interpret</li>\n",
    "    <li>if the right subset is chosen, it can improve the accuracy of a model</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Choosing the right subset often relies on domain knowledge, some art, and a bit of luck.</strong>\n",
    "<p>\n",
    "    In our dataset, we happen to notice that features with \"worst\" seem to have higher importances. As a result we are going to build a new model below with the selected features and see if it improves accuracy.\n",
    "</p>\n",
    "<p>We first find the features whose names include the word \"worst\":</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c3593e-528a-40e4-adf9-c88423011496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "worst_cols = [col for col in df.columns if 'worst' in col]\n",
    "print(worst_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113df6a2-8fc4-4198-9855-b45ca6c82566",
   "metadata": {},
   "source": [
    "<p>\n",
    "    There are ten such features. Now we create another dataframe with the selected features, followed by a train test split with the same random state.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853d63d0-0c88-4b94-9efa-5cd66935eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_worst = df[worst_cols]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_worst, y, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6444b4b-9b82-4689-a497-c2db2c9443b0",
   "metadata": {},
   "source": [
    "<p>At the end, we fit the model and output the accuracy.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "488638a1-3b9e-4138-930c-676239e5243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=101)\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074febee-9a88-4d7e-8706-f67efd3ba25a",
   "metadata": {},
   "source": [
    "<strong>\n",
    "    Here we weren't able to improve the accuracy by much using a subset of features.<br/>\n",
    "    But giving that we only used a third of the total features and removed some noise and highly correlated features, we get the advantage of building a better model using less features that will be more pronounced when the sample size is large.\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1830f5-24b9-4f2f-bcd3-7bbf18de943a",
   "metadata": {},
   "source": [
    "<strong>Some conclusions around feature selection</strong>\n",
    "<ul>\n",
    "    <li>There is no best feature selection method, at least not universally.</li>\n",
    "    <li>Instead, we must discover what works best for the specific problem and leverage the domain expertise to build a good model.</li>\n",
    "    <li>Scikit-learn provides an easy way to discover the feature importances.</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
